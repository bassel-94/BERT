{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "functions_postprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXjkhTQOq92/yyxhggqEf/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bassel-94/BERT/blob/main/code/functions_postprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_soQxSj1c3AP"
      },
      "source": [
        "# Post processing function for camemBERT\n",
        "\n",
        "* get_numpies for computing numpy arrays (dimensions are n$\\times$18)\n",
        "* get_predictions for label predictions using a probability threshold\n",
        "* get_n_label_predictions \n",
        "* undummify data\n",
        "* get_sentiment_prediction\n",
        "* get_random_split\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibC3P-Ymcv9q"
      },
      "source": [
        "def get_predictions(df, predictor, prob = 0.5):\n",
        "  '''\n",
        "  function to add a column called Labels_predicted to the dataframe df\n",
        "  df must have a \"comments\" column\n",
        "  \n",
        "  inputs:\n",
        "  *******\n",
        "  df : data frame that contains column \"comments\"\n",
        "  predictor : the predictor class of fast_bert\n",
        "  prob : probability threshold to display a label (default 0.5)\n",
        "\n",
        "  output:\n",
        "  *******\n",
        "  original data frame with an extra column called Labels_predicted\n",
        "  '''\n",
        "\n",
        "  # loop over the dataframe we want to label\n",
        "  l = []\n",
        "  for index, row in df.iterrows():\n",
        "    \n",
        "    # get predictions for each row that contains a comment\n",
        "    pred = predictor.predict(row[\"comments\"])\n",
        "    \n",
        "    # loop over the found labels with probabiliry \"prob\" and concatenate them\n",
        "    a = []\n",
        "    [a.append(i[0]) if i[1]>prob else None for i in pred ]\n",
        "    \n",
        "    # join predicted values\n",
        "    Labels_predicted = \", \".join(a)\n",
        "    \n",
        "    # append predicted values in a list\n",
        "    l.append(Labels_predicted)\n",
        "\n",
        "  # add predicted labels to a column in the original data frame\n",
        "  df[\"Labels_predicted\"] = l\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG1u51CzdVYO"
      },
      "source": [
        "# function that detects if all labels have been predicted or not and returns full numpies.\n",
        "def get_numpies(df_true, df_pred):\n",
        "  '''\n",
        "  function that takes two data frames as input and checks whether all \n",
        "  labels have been predicted or not. If there are inconsistencies with\n",
        "  the dimensions, this means not all labels have been detected and we \n",
        "  need to fill the data frame with zeros for the non predicted labels.\n",
        "  If not, this means that all labels have been detected and we compute numpies\n",
        "\n",
        "  inputs:\n",
        "  *******\n",
        "  df_true : dataframe of the true labels in the form of dummies\n",
        "  df_pred : data frame of predicted labels in the form of joined strings. The\n",
        "  predicted labels should be in the last column of the dataframe.\n",
        "\n",
        "  output:\n",
        "  *******\n",
        "  y_true, y_pred the predicted and the true labels in the form of dummified \n",
        "  numpy arrays. their dimensions are length of the dataframe x number of labels.\n",
        "  '''\n",
        "  \n",
        "  # condition to know if the data frame has already been dummified (len >3) or not (len <3)\n",
        "  if len(df_true.columns) <= 3:\n",
        "    df_true_dummies = df_true[\"Labels\"].str.get_dummies(sep = \", \")\n",
        "    df_true = df_true.join(df_true_dummies).drop(\"Labels\", axis = 1)\n",
        "\n",
        "  # get true label matrix (dummified) as numpy array\n",
        "  y_true = df_true.iloc[:, 1:].to_numpy()\n",
        "  \n",
        "  # get list of predicted labels and true labels\n",
        "  list_pred = df_pred[df_pred.columns[-1]].str.get_dummies(sep=', ').columns.tolist()\n",
        "  list_true = df_true.iloc[:, 1:].columns.tolist()\n",
        "\n",
        "  # check whether there are labels that have not been predicted\n",
        "  left_over = [x for x in list_true if x not in list_pred]\n",
        "\n",
        "  # if the list if not empty (i.e. some labels are not detected)\n",
        "  if left_over:\n",
        "\n",
        "    # print the left over labels that have not been detected.\n",
        "    print(\"The list is not empty. The following labels have not been detected:\", \", \".join(left_over))\n",
        "\n",
        "    # create data frame of the dummy labels that were not detected and fill with zeros\n",
        "    df_rest = pd.DataFrame(0, columns=left_over, index=np.arange(len(df_true)))\n",
        "  \n",
        "    # compute again the predicted numpy array with the right dimensions and order.\n",
        "    y_pred = (df_pred[df_pred.columns[-1]].str\n",
        "                                .get_dummies(sep=', ')\n",
        "                                .join(df_rest)\n",
        "                                .sort_index(axis = 1)\n",
        "                                .fillna(0)\n",
        "                                .to_numpy())\n",
        "  \n",
        "    # make sure predicted and true labels have the same dimensions\n",
        "    print(\"Dimension of the true dummies:\", y_true.shape)\n",
        "    print(\"Dimension of the pred dummies after adjustment:\", y_pred.shape)\n",
        "\n",
        "  # if the list is empty (i.e. all labels have been detected)\n",
        "  else :\n",
        "\n",
        "    # print message saying that all labels have been detected\n",
        "    print(\"No need for adjustment, all labels have been detected!\")\n",
        "\n",
        "    # get predicted labels (dummified) as numpy array\n",
        "    y_pred = df_pred[df_pred.columns[-1]].str.get_dummies(sep=', ').to_numpy()\n",
        "      \n",
        "  return y_true, y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh91FopBdplv"
      },
      "source": [
        " # wrap it up in a function\n",
        " def get_n_labels_predictions(df_true, n_labels_list, predictor, prob=0.4):\n",
        "   '''\n",
        "   function that takes as input the two dataframes; true and predicted\n",
        "   and returns the roc error according to the number of labels\n",
        "\n",
        "   inputs:\n",
        "   *******\n",
        "\n",
        "   outputs:\n",
        "   ********\n",
        "\n",
        "   '''\n",
        "   # add column called num_labels to get sum of number of labels per row\n",
        "   df_true_sum = (df_true.iloc[:, 1:]\n",
        "                  .sum(axis=1)\n",
        "                  .to_frame()\n",
        "                  .rename(columns={0: \"num_labels\"})\n",
        "                  .join(df_true))\n",
        "   \n",
        "   # loop over the list of selected number of labels\n",
        "   l = []\n",
        "   for i in n_labels_list:\n",
        "     \n",
        "     # get new data frame that contains n labels\n",
        "     df_true_sum_n = df_true_sum[df_true_sum[\"num_labels\"]==i].iloc[:, 1:]\n",
        "\n",
        "     # get the true labels as a list. Only select non zero columns of the true dataframe.\n",
        "     labels = df_true_sum_n.iloc[:, 1:].columns.tolist()\n",
        "     #list(df_true_sum_n.loc[:, df_true_sum_n.any()].columns[1:])\n",
        "\n",
        "     # get predictions for the n labeled reviews\n",
        "     df_pred_sum_n = get_predictions(df_true_sum_n[[\"comments\"]], predictor=predictor, prob=prob)\n",
        "\n",
        "     # get numpy arrays for the predicted and true labels\n",
        "     print(\"for reviews that have\", i, \"labels:\")\n",
        "     y_true_sum_n, y_pred_sum_n = get_numpies(df_true = df_true_sum_n, df_pred = df_pred_sum_n)\n",
        "\n",
        "     # print roc score\n",
        "     roc_n = roc_auc_score(y_true_sum_n, y_pred_sum_n, average = \"micro\")\n",
        "     print(\"ROC_AUC score :\", roc_n, \"\\n\")\n",
        "\n",
        "     # get classification report of all labels\n",
        "     cl_report = classification_report(y_true_sum_n, y_pred_sum_n, target_names = labels)\n",
        "\n",
        "     # save all results in a list for later use\n",
        "     l.append([roc_n, cl_report])\n",
        "   \n",
        "   return l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wItir6aEsXdq"
      },
      "source": [
        "def get_count(df, display_plot = False):\n",
        "  '''\n",
        "  function to compute a bar plot of the label counts of the data\n",
        "  frame df. the data frame should have \"Count\" and \"Label\" columns.\n",
        "  '''\n",
        "  \n",
        "  # get dummies\n",
        "  df_dummies = df.iloc[:, 1:]\n",
        "  \n",
        "  # get stats\n",
        "  df_stats = (df_dummies\n",
        "        .sum(axis=0)\n",
        "        .to_frame()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"index\": \"Label\", 0: \"Count\"})\n",
        "        .sort_values(\"Count\", ascending = False)\n",
        "        .reset_index(drop=True))\n",
        "        \n",
        "  # barplot of label count\n",
        "  if display_plot:\n",
        "\n",
        "    # import plot packages\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    \n",
        "    # set figure size\n",
        "    plt.figure(figsize=(12, 7));\n",
        "    g = sns.barplot(x=\"Count\", y=\"Label\", data=df_stats, color = \"steelblue\")\n",
        "    \n",
        "    # add annotations to the barplot\n",
        "    for p in g.patches:\n",
        "      width = p.get_width()\n",
        "      plt.text((df_stats.Count.max()/70) + p.get_width(), p.get_y()+0.55*p.get_height(), round(width), ha='center', va='center')\n",
        "      \n",
        "    # add titles\n",
        "    plt.xlabel(\"Count\");\n",
        "    plt.ylabel(\"Labels\");\n",
        "    plt.title(\"Bar plot of label count. Total num of reviews is \" +str(len(df)));\n",
        "    plt.xticks(fontsize=12);\n",
        "    plt.yticks(fontsize=12);\n",
        "    plt.tight_layout();\n",
        "    plt.show();\n",
        "  \n",
        "  return df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvhNWu1yhviZ"
      },
      "source": [
        "# function to get a random train test split\n",
        "def get_random_split(df, test_size, seed=42):\n",
        "  '''\n",
        "  function to get a random train test split of a data frame according to\n",
        "  test_size, seed and without repetition \n",
        "  \n",
        "  inputs:\n",
        "  *******\n",
        "  df : dataframe to split\n",
        "  test_size : the proportion of test size to take\n",
        "  seed : random seed for reproducibility\n",
        "  '''\n",
        "  \n",
        "  # train validation split \n",
        "  val_set = df.sample(frac = test_size, replace=False, random_state=seed)\n",
        "  train_set = df.drop(index = val_set.index)\n",
        "  \n",
        "  # print message displaying number of labels and rows in each set\n",
        "  print(\"there are\", len(train_set), \"rows in the train set with\", len(train_set.iloc[:, 1:].columns) , \"labels\")\n",
        "  print(\"there are\", len(val_set), \"rows in the validation set with\", len(val_set.iloc[:, 1:].columns), \"labels\")\n",
        "  \n",
        "  return val_set, train_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ytR4c89HFrr"
      },
      "source": [
        "# function to reverse the dummified categorical variables in a dataframe \n",
        "def undummify(df, sep = \", \"):\n",
        "  '''\n",
        "  function that transforms the dummified data frame back into a multilabel\n",
        "  column with the same separator\n",
        "  \n",
        "  inputs :\n",
        "  ********\n",
        "  df : data frame whose first three columns are the ones we want to keep. After the \n",
        "  first three columns we consider that we only have dummified categorical variables.\n",
        "  sep : The separator to use when concatenating the Labels\n",
        "  \n",
        "  output:\n",
        "  *******\n",
        "  df_melted : dataframe that should be identical to the one before applying the function\n",
        "  get_dummies(). The seperator of the categorical variables is by default ', '\n",
        "  '''\n",
        "  \n",
        "  # get data frame that contains only the dummies\n",
        "  df_dummies = df.loc[:, df.columns != 'comments']\n",
        "  \n",
        "  # melt the dummies into a data frame with one column and a separator\n",
        "  df_labels = (df_dummies\n",
        "         .dot(df_dummies.columns + sep)\n",
        "         .str.rstrip(sep)\n",
        "         .to_frame()\n",
        "         .rename(columns={0: \"Labels\"}))\n",
        "  \n",
        "  # join the comments column to the newly created Labels data frame\n",
        "  df_melted = df.loc[:, df.columns == 'comments'].join(df_labels)\n",
        "  \n",
        "  return df_melted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6tx1OeQyKGX"
      },
      "source": [
        "# function to get sentiment prediction\n",
        "def get_sentiment_prediction(df, model, tokenizer):\n",
        "  '''\n",
        "  function that computes predictions for positive and negative sentiment\n",
        "  on a dataframe that contains a columns \"comments\".\n",
        "  \n",
        "  inputs:\n",
        "  *******\n",
        "  df: a data frame with columns called \"comments\"\n",
        "  model: the camembert model used to make the prediction\n",
        "  tokenizer: the tokenizer used for the specified model\n",
        "  \n",
        "  output:\n",
        "  *******\n",
        "  df: the same dataframe as the input with an extra column of predicted sentiment\n",
        "  '''\n",
        "  \n",
        "  # define pipeline based on the model and tokenizer\n",
        "  nlp = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
        "  \n",
        "  # define empty list to concatenate predictions\n",
        "  predictions, proba = [], []\n",
        "\n",
        "  # iterate over datafame\n",
        "  for index, row in df.iterrows():\n",
        "    \n",
        "    # apply nlp pipeline that contains the predict method onr comments\n",
        "    pred_dict = nlp(row[\"comments\"])[0]   # returns a dictionnary\n",
        "    pred_label = pred_dict[\"label\"]       # extracts the value of the key \"label\"\n",
        "    pred_proba = pred_dict[\"score\"]       # extract the value of the key \"score\"\n",
        "    \n",
        "    # append to list\n",
        "    predictions.append(pred_label)\n",
        "    proba.append(pred_proba)\n",
        "    \n",
        "  # add results to the dataframe in a new column\n",
        "  df[\"predicted_sentiment\"] = predictions\n",
        "  df[\"predicted_probability\"] = proba\n",
        "  \n",
        "  return(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYuUxuHFDP6j"
      },
      "source": [
        "def get_shiny_predictions(df, predictor, prob = 0.5):\n",
        "  '''\n",
        "  function to add a column called Labels_predicted to the dataframe df.\n",
        "  It includes the probability of each label in each row of the dataframe.\n",
        "  df must have a \"comments\" column!\n",
        "  \n",
        "  inputs:\n",
        "  *******\n",
        "  df : data frame that contains column \"comments\"\n",
        "  predictor : the predictor class of fast_bert\n",
        "  prob : probability threshold to display a label (default 0.5)\n",
        "\n",
        "  output:\n",
        "  *******\n",
        "  original data frame with an extra column called Labels_predicted\n",
        "  '''\n",
        "\n",
        "  # loop over the dataframe we want to label\n",
        "  l = []\n",
        "  for index, row in df.iterrows():\n",
        "    \n",
        "    # get predictions for each row that contains a comment\n",
        "    pred = predictor.predict(row[\"comments\"])\n",
        "    \n",
        "    # loop over the found labels with probabiliry \"prob\" and concatenate them\n",
        "    a = []\n",
        "    [a.append(i) if i[1]>=prob else None for i in pred ]\n",
        "    \n",
        "    # append predicted values in a list\n",
        "    l.append(a)\n",
        "\n",
        "  # add predicted labels to a column in the original data frame\n",
        "  df[\"Labels_predicted\"] = l\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}